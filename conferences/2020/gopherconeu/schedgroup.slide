schedgroup: a timer-based goroutine concurrency primitive
16 Jun 2020

Matt Layher
Distributed Systems Engineer, Fastly
mdlayher@gmail.com
https://github.com/mdlayher
https://mdlayher.com
@mdlayher

* Disclaimer

I work for Fastly, but this work is not related to Fastly in any way.

* Matt Layher

@mdlayher, mdlayher.com, github.com/mdlayher/talks

.image colorado-square.jpg 500 _

* Introducing schedgroup

A package that allows scheduling or delaying a goroutine to run at or after a
specific point in time.

.play basic/main.go /START OMIT/,/END OMIT/

.link https://github.com/mdlayher/schedgroup

* Why?

Typically you want your code to run immediately and as fast as possible.

- Throttling or delaying an aggressive client's requests

- Sometimes you don't make the rules!

* Introducing CoreRAD

CoreRAD is an extensible and observable IPv6 Neighbor Discovery Protocol (NDP) 
router advertisement daemon.

.link https://github.com/mdlayher/corerad

- Allows IPv6 clients to find routers and establish a default route
- Enables IPv6 Stateless Address Autoconfiguration (SLAAC)

CoreRAD uses NDP (RFC 4861) to perform these functions:

.link https://tools.ietf.org/html/rfc4861

Learn more about IPv6 and NDP:

.link https://mdlayher.com/talks

* NDP rules

RFC 4861 mandates that IPv6 routers rate limit both solicited and unsolicited
router advertisements.

When multicasting to the IPv6 link-local all nodes address:

    ... consecutive Router Advertisements sent to the all-nodes multicast address MUST be rate
    limited to no more than one advertisement every MIN_DELAY_BETWEEN_RAS seconds.

And even when unicasting a response to individual router solicitations:

    In all cases, Router Advertisements sent in response to a Router Solicitation MUST be
    delayed by a random time between 0 and MAX_RA_DELAY_TIME seconds.

* time.AfterFunc: using the standard library

`time.AfterFunc` does exactly what we need! Problem solved?

.play naive/main.go /START OMIT/,/END OMIT/

* Cancelation support: the naive prototype

Can we add context cancelation?

.play naivecontext/main.go /START OMIT/,/END OMIT/

* Cancelation support: the naive prototype

We can use pprof to examine our program with a nice web UI:

    $ go tool pprof -http :8080 localhost:8081/debug/pprof/goroutine

* Cancelation support: the naive prototype

.image naivecontext/goroutines.png 550 _

* Cancelation support: the naive prototype

No library, the caller handles goroutines and timers.

Pros:

- Simplicity: very little code

Cons:

- Caller has to manage concurrency and cancelation
- Unbounded number of goroutines, wasteful use of memory
- Excessive number of runtime timers

* Efficiency vs. convenience

Can we write efficient and tidy code which also supports cancelation?

- `time.AfterFunc` is well-optimized, but inflexible
- Spinning up a goroutine to wait for a timer or cancelation is wasteful

* schedgroup: worker pool

A library with a fixed number of worker goroutines consuming work from a channel.

Goals:

- Bounded number of goroutines
- Support for context cancelation

* schedgroup: worker pool

A Group coordinates concurrent workers to expose a concise public API.

.code workerpool/schedgroup/group.go /START 1 OMIT/,/END 1 OMIT/

* schedgroup: worker pool

Tasks are created by Delay, and the caller can block until completion by calling Wait.

.code workerpool/schedgroup/group.go /START 2 OMIT/,/END 2 OMIT/

* schedgroup: worker pool

Internal worker goroutines consume tasks and execute them when a delay elapses.

.code workerpool/schedgroup/group.go /START 3 OMIT/,/END 3 OMIT/

* schedgroup: worker pool

Demo appears and behaves identically to the completed version earlier in the slides.

.play workerpool/main.go /START OMIT/,/END OMIT/

* schedgroup: worker pool

.image workerpool/goroutines.png 550 _

* schedgroup: worker pool

A library with a fixed number of worker goroutines consuming work from a channel.

Pros:

- Concurrency and cancelation are managed internally
- Bounded number of goroutines
- Reasonably concise code thanks to Go's concurrency primitives

Cons:

- Idle worker goroutines consume system resources
- An overwhelming amount of tasks could result in blocking the caller

* schedgroup: timer-polling using a monitor goroutine

A library which polls continuously to check for timer expiration.

Goals:

- Don't spin up goroutines until tasks are ready
- Support for context cancelation
- Efficient scheduling of tasks which are ready

* Introducing container/heap

A min-heap will allow us to quickly determine which tasks should be scheduled first.

    // A type, typically a collection, that satisfies sort.Interface can be sorted
    // by the routines in container/heap.
    type Interface interface {
        // sort.Interface
        Len() int
        Less(i, j int) bool
        Swap(i, j int)

        Push(x interface{}) // add x as element Len()
        Pop() interface{}   // remove and return element Len() - 1.
    }

Let's implement 'heap.Interface' for a slice of tasks.

* Implementing heap.Interface

.code timerpolling/schedgroup/group.go /START TASK OMIT/,/END TASK OMIT/

* container/heap usage

container/heap functions maintain the min-heap's structure.

.play timerpolling/heap.go /START HEAP OMIT/,/END HEAP OMIT/

* schedgroup: timer-polling using a monitor goroutine

A Group coordinates concurrent workers to expose a concise public API.

.code timerpolling/schedgroup/group.go /START GROUP OMIT/,/END GROUP OMIT/

* schedgroup: timer-polling using a monitor goroutine

The Delay method makes a return, but is now a wrapper for the generalized Schedule.

.code timerpolling/schedgroup/group.go /START DELAY OMIT/,/END DELAY OMIT/

* schedgroup: timer-polling using a monitor goroutine

The monitor goroutine is responsible for ticking every millisecond to see if
the tasks heap has any scheduled tasks ready.

.code timerpolling/schedgroup/group.go /START MONITOR OMIT/,/END MONITOR OMIT/

* schedgroup: timer-polling using a monitor goroutine

The trigger method will see if any work is ready to be invoked as of time 'now'.

.code timerpolling/schedgroup/group.go /START TRIGGER OMIT/,/END TRIGGER OMIT/

* schedgroup: timer-polling using a monitor goroutine

Wait blocks until work is done or the Group's context is canceled.

.code timerpolling/schedgroup/group.go /START WAIT OMIT/,/END WAIT OMIT/

* schedgroup: timer-polling using a monitor goroutine

Demo appears and behaves identically to both previous versions.

.play timerpolling/main.go /START OMIT/,/END OMIT/

* schedgroup: timer-polling using a monitor goroutine

I deployed this code and noticed a significant CPU usage increase.

What's happening? Let's try out 'go tool trace'.

    $ curl http://localhost:8081/debug/pprof/trace > trace.out
    $ go tool trace trace.out

Note the subtle red/pink banding in the PROCS section.

.image timerpolling/trace.png 250 _

* schedgroup: timer-polling using a monitor goroutine

Focusing on a 100ms slice shows the problem more clearly:

- Polling the tasks heap every 1ms resulting in excessive CPU usage
- Ideally we want sustained CPU usage or no CPU usage; this is just inefficient!

.image timerpolling/tracezoom.png 250 _

* schedgroup: timer-polling using a monitor goroutine

A library which polls continuously to check for timer expiration.

Pros:

- Concurrency and cancelation are managed internally
- Bounded number of goroutines

Cons:

- Extremely inefficient use of CPU due to wasteful timer polling
- Both Group.monitor and Group.Wait must poll to wait for completion

* schedgroup: event-driven scheduler with signaling channels

A library which uses goroutines, channels, and select to efficiently signal
when work is ready.

Goals:

- Don't spin up goroutines until tasks are ready
- Support for context cancelation
- Efficient scheduling of tasks which are ready
- No polling of timers to avoid excessive CPU usage

This is the final result of my experiments:

.link https://github.com/mdlayher/schedgroup

* schedgroup: event-driven scheduler with signaling channels

Some parts of the previous design are retained:

- Min-heap of tasks ordered by their scheduled time
- Monitor goroutine to schedule tasks when they are ready to run

* schedgroup: event-driven scheduler with signaling channels

A Group coordinates concurrent workers to expose a concise public API.

.code final/schedgroup/group.go /START GROUP OMIT/,/END GROUP OMIT/

* schedgroup: event-driven scheduler with signaling channels

Delay and Schedule now signal that work has been added to the heap.

.code final/schedgroup/group.go /START SCHEDULE OMIT/,/END SCHEDULE OMIT/

* schedgroup: event-driven scheduler with signaling channels

This is a non-blocking send attempt on g.addC:

- If a value can be sent on g.addC, the value is sent.
- Otherwise, the default case is selected and nothing happens.

    // Notify monitor that a new task has been pushed on to the heap.
    select {
    case g.addC <- struct{}{}:
        // empty struct is sent
    default:
        // nothing happens
    }

Why an empty struct? Some prefer using a bool.

I prefer the empty struct because it indicates that the value sent is truly
meaningless and cannot be interpreted in any way.

* schedgroup: event-driven scheduler with signaling channels

The monitor goroutine runs a single timer which waits until the earliest
scheduled event is ready to fire.

.code final/schedgroup/group.go /START MONITOR1 OMIT/,/END MONITOR1 OMIT/

* schedgroup: event-driven scheduler with signaling channels

Once tasks are scheduled, we wait until a new event occurs.

.code final/schedgroup/group.go /START MONITOR2 OMIT/,/END MONITOR2 OMIT/

If tickC is nil because there are no more tasks waiting to be scheduled, that
select case can be "shut off".

* schedgroup: event-driven scheduler with signaling channels

Demo appears and behaves identically to all previous versions.

.play final/main.go /START OMIT/,/END OMIT/


* schedgroup: event-driven scheduler with signaling channels

Pros:

- Concurrency and cancelation are managed internally
- Single timer heap management goroutine
- Goroutines are spun up on-demand exactly when needed
- No polling!

Cons:

- Internal complexity: it took me a while to work out bugs
- Potential for deadlocks due to subtle concurrency bugs

* schedgroup: naive prototype vs. event-driven scheduler

Let's do a head-to-head comparison of our naive prototype vs. the final implementation.

We will examine:

- number of running goroutines
- in-use heap space

* schedgroup: naive prototype vs. event-driven scheduler

Increment a counter to 1 million after ~10 seconds per task.

.code final/finalpprof/main.go /START OMIT/,/END OMIT/

* schedgroup goroutines: naive prototype

.image naivecontext/goroutines.png 550 _

* schedgroup goroutines: event-driven scheduler

Massive reduction in number of goroutines: ~1,000,000 to 3

.image final/goroutines.png 450 _

* schedgroup heap in-use space: naive prototype

    total:          219.74MB
    
    runtime.mstart: 148.10MB
    time.After:      38.64MB
    other:           33.00MB

.image naivecontext/heapinuse.png 150 _

* schedgroup heap in-use space: event-driven scheduler

Roughly 6x reduction in in-use heap bytes: 219.74MB to 36.50MB

    total:                     36.50MB

    schedgroup.(*Group).Delay: 30.50MB
    other:                      6.00MB

.image final/heapinuse.png 180 _

* Summary

What did we learn?

- Proper measurement is essential to prove or disprove a performance hypothesis
- Focus on correctness and simplicity, then make iterative improvements
- Go runtime timers are very efficient, especially as of Go 1.14

.link https://golang.org/doc/go1.14#runtime

* More information

The final package is available on GitHub:

.link https://github.com/mdlayher/schedgroup

Special thanks to Egon Elbre for two initial prototypes which heavily influenced
the final design of the package.

.link https://twitter.com/egonelbre

Go issue: runtime: make timers faster

.link http://golang.org/issue/6239

CoreRAD: an extensible and observable IPv6 NDP router advertisement daemon

.link https://github.com/mdlayher/corerad

.link https://mdlayher.com/blog/corerad-a-new-ipv6-router-advertisement-daemon/
